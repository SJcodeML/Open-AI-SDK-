{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3nHDTO095FLoNlP2GdE4s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SJcodeML/Open-AI-SDK-/blob/main/class_12_%2C13%2C14%2C15Open_AI_SDK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=JV3DbNn83YI&list=PL0vKVrkG4hWovpr0FX6Gs-06hfsPDEUe6&index=12\n",
        "\n",
        "\n",
        "# Above link is of the video link where i practice from the below code"
      ],
      "metadata": {
        "id": "JGjoAQClS2SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq openai-agents  \"openai-agents[litellm]\""
      ],
      "metadata": {
        "id": "1ZWo66Er5J_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9abbbc4b-7082-4d67-c252-91910bd9242a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m718.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.3/164.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.1/137.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhuG1f3P0QKX"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import asyncio\n",
        "\n",
        "from agents import Agent,Runner,function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "@function_tool\n",
        "def get_weat (city:str)->str:\n",
        "  print(f\"[debug] {city}\")\n",
        "  return f\"The weather in {city} is sunny\"\n",
        "\n",
        "weather_agent = Agent(\n",
        "      name= \"Weather Assistant\",\n",
        "      instructions= \"You only respond in haikus\",\n",
        "      model =LitellmModel(model=MODEL,api_key=GEMINI_API_KEY),\n",
        "      tools= [get_weat],\n",
        "      handoff_description = \"Weather Assistant is responsible for all weather queries \"\n",
        "\n",
        "  )\n",
        "\n",
        "\n",
        "panaversity_agent = Agent(\n",
        "      name= \"Panaversity Assistant \",\n",
        "      instructions= \"you will answer all Panaversity related queries\",\n",
        "      model =LitellmModel(model=MODEL,api_key=GEMINI_API_KEY),\n",
        "      handoff_description = \"Panaversity Assistant is responsible for all Panaversity queries \"\n",
        "  )\n",
        "\n",
        "triage_agent = Agent(\n",
        "    name= \"Triage Assistant\",\n",
        "    instructions = \"You will chat with the user and extract the city name if they are asking about weather and when user ask a question about weather you will handoff to the Weather Assistant or user ask about panaversity you will handoff to the  panaversity assistant \",\n",
        "    model =LitellmModel(model = MODEL , api_key = GEMINI_API_KEY),\n",
        "    handoffs = [weather_agent, panaversity_agent ],\n",
        "    # handoff_description = \"you need to handoff to weather assistant when question is asked about the weather ,if question is related to panaversity so u must handoff to the panaversity agent \"\n",
        "    # tools=[weather_agent.as_tool(tool_name=\"Weather Tool\", tool_description=\"Weather Assistant is responsible for all weather queries \")]\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "result = Runner.run_sync(triage_agent,\"tell me something about panaversity\" )\n",
        "print(result.final_output)\n",
        "print(result.last_agent.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTu5lY8K5yAM",
        "outputId": "e811a323-4cff-4222-e315-04e44e3007fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am the Panaversity Assistant, ready to help you with any questions you have about Panaversity. What would you like to know?\n",
            "\n",
            "Panaversity Assistant \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = Runner.run_sync(triage_agent,\"when panaverity start?\" )\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE9gFrAM-K8I",
        "outputId": "249240eb-026c-4bde-b924-bb131eb2dce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can help you with Panaversity related queries. Could you please be more specific about what you'd like to know?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = Runner.run_sync(triage_agent,\"what is the weather in karachi\" )\n",
        "print(result.final_output)\n",
        "print(result.last_agent.name)"
      ],
      "metadata": {
        "id": "nuAerKJ6aD7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b1f1fd6-2dba-4058-af4c-8ff88d7ca239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I cannot access that.\n",
            "I can get weather data\n",
            "If you give the city.\n",
            "\n",
            "Weather Assistant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import asyncio\n",
        "\n",
        "\n",
        "from agents import Agent,Runner,function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "@function_tool\n",
        "def get_weat (city:str)->str:\n",
        "  print(f\"[debug] {city}\")\n",
        "  return f\"The weather in {city} is sunny\"\n",
        "\n",
        "weather_agent = Agent(\n",
        "      name= \"Weather Assistant\",\n",
        "      instructions= \"You only respond in haikus\",\n",
        "      model =LitellmModel(model=MODEL,api_key=GEMINI_API_KEY),\n",
        "      tools= [get_weat],\n",
        "      handoff_description = \"Weather Assistant is responsible for all weather queries \"\n",
        "\n",
        "  )\n",
        "\n",
        "\n",
        "panaversity_agent = Agent(\n",
        "      name= \"Panaversity Assistant \",\n",
        "      instructions= \"you will answer all Panaversity related queries\",\n",
        "      model =LitellmModel(model=MODEL,api_key=GEMINI_API_KEY),\n",
        "      handoff_description = \"Panaversity Assistant is responsible for all Panaversity queries \"\n",
        "  )\n",
        "\n",
        "triage_agent = Agent(\n",
        "    name= \"Triage Assistant\",\n",
        "    instructions = \"You will chat with the user and extract the city name if they are asking about weather and when user ask a question about weather you will handoff to the Weather Assistant or user ask about panaversity you will handoff to the  panaversity assistant \",\n",
        "    model =LitellmModel(model = MODEL , api_key = GEMINI_API_KEY),\n",
        "    handoffs = [weather_agent, panaversity_agent ],\n",
        "    # handoff_description = \"you need to handoff to weather assistant when question is asked about the weather ,if question is related to panaversity so u must handoff to the panaversity agent \"\n",
        "    # tools=[weather_agent.as_tool(tool_name=\"Weather Tool\", tool_description=\"Weather Assistant is responsible for all weather queries \")]\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "result = Runner.run_sync(triage_agent,\"how is the weather in karachi\" )\n",
        "print(result.final_output)\n",
        "print(result.last_agent.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBM7RwHX1WVA",
        "outputId": "5bdf3ab9-2394-41e7-e6ad-53cc39cac9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I cannot help yet.\n",
            "I need other tools.\n",
            "Perhaps try again?\n",
            "\n",
            "Weather Assistant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agents.extensions.visualization import draw_graph\n",
        "draw_graph(triage_agent)"
      ],
      "metadata": {
        "id": "AJVDs2ge_lpn",
        "outputId": "288a4673-a929-4756-b2c3-14ab3b07e260",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"300pt\" height=\"297pt\"\n viewBox=\"0.00 0.00 300.00 297.05\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 293.05)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-293.05 296,-293.05 296,4 -4,4\"/>\n<!-- __start__ -->\n<g id=\"node1\" class=\"node\">\n<title>__start__</title>\n<ellipse fill=\"lightblue\" stroke=\"black\" cx=\"151.5\" cy=\"-272.79\" rx=\"51.74\" ry=\"16.03\"/>\n<text text-anchor=\"middle\" x=\"151.5\" y=\"-269.09\" font-family=\"Arial\" font-size=\"14.00\">__start__</text>\n</g>\n<!-- Triage Assistant -->\n<g id=\"node3\" class=\"node\">\n<title>Triage Assistant</title>\n<polygon fill=\"lightyellow\" stroke=\"black\" points=\"208,-220.53 95,-220.53 95,-162.53 208,-162.53 208,-220.53\"/>\n<text text-anchor=\"middle\" x=\"151.5\" y=\"-187.83\" font-family=\"Arial\" font-size=\"14.00\">Triage Assistant</text>\n</g>\n<!-- __start__&#45;&gt;Triage Assistant -->\n<g id=\"edge1\" class=\"edge\">\n<title>__start__&#45;&gt;Triage Assistant</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M151.5,-256.51C151.5,-249.12 151.5,-239.89 151.5,-230.76\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"155,-230.6 151.5,-220.6 148,-230.6 155,-230.6\"/>\n</g>\n<!-- __end__ -->\n<g id=\"node2\" class=\"node\">\n<title>__end__</title>\n<ellipse fill=\"lightblue\" stroke=\"black\" cx=\"93.5\" cy=\"-16.26\" rx=\"48.58\" ry=\"16.03\"/>\n<text text-anchor=\"middle\" x=\"93.5\" y=\"-12.56\" font-family=\"Arial\" font-size=\"14.00\">__end__</text>\n</g>\n<!-- Weather Assistant -->\n<g id=\"node4\" class=\"node\">\n<title>Weather Assistant</title>\n<path fill=\"none\" stroke=\"black\" d=\"M280,-126.53C280,-126.53 179,-126.53 179,-126.53 173,-126.53 167,-120.53 167,-114.53 167,-114.53 167,-80.53 167,-80.53 167,-74.53 173,-68.53 179,-68.53 179,-68.53 280,-68.53 280,-68.53 286,-68.53 292,-74.53 292,-80.53 292,-80.53 292,-114.53 292,-114.53 292,-120.53 286,-126.53 280,-126.53\"/>\n<text text-anchor=\"middle\" x=\"229.5\" y=\"-93.83\" font-family=\"Arial\" font-size=\"14.00\">Weather Assistant</text>\n</g>\n<!-- Triage Assistant&#45;&gt;Weather Assistant -->\n<g id=\"edge2\" class=\"edge\">\n<title>Triage Assistant&#45;&gt;Weather Assistant</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M175.33,-162.42C182.82,-153.59 191.19,-143.71 199.07,-134.41\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"201.75,-136.67 205.55,-126.77 196.41,-132.14 201.75,-136.67\"/>\n</g>\n<!-- Panaversity Assistant  -->\n<g id=\"node6\" class=\"node\">\n<title>Panaversity Assistant </title>\n<path fill=\"none\" stroke=\"black\" d=\"M137,-126.53C137,-126.53 12,-126.53 12,-126.53 6,-126.53 0,-120.53 0,-114.53 0,-114.53 0,-80.53 0,-80.53 0,-74.53 6,-68.53 12,-68.53 12,-68.53 137,-68.53 137,-68.53 143,-68.53 149,-74.53 149,-80.53 149,-80.53 149,-114.53 149,-114.53 149,-120.53 143,-126.53 137,-126.53\"/>\n<text text-anchor=\"middle\" x=\"74.5\" y=\"-93.83\" font-family=\"Arial\" font-size=\"14.00\">Panaversity Assistant </text>\n</g>\n<!-- Triage Assistant&#45;&gt;Panaversity Assistant  -->\n<g id=\"edge6\" class=\"edge\">\n<title>Triage Assistant&#45;&gt;Panaversity Assistant </title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M127.98,-162.42C120.66,-153.68 112.48,-143.91 104.77,-134.69\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"107.24,-132.2 98.14,-126.77 101.88,-136.69 107.24,-132.2\"/>\n</g>\n<!-- Weather Assistant&#45;&gt;__end__ -->\n<g id=\"edge5\" class=\"edge\">\n<title>Weather Assistant&#45;&gt;__end__</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M181.17,-68.36C162.62,-57.55 141.93,-45.49 125.31,-35.8\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"126.93,-32.7 116.53,-30.69 123.41,-38.75 126.93,-32.7\"/>\n</g>\n<!-- get_weat -->\n<g id=\"node5\" class=\"node\">\n<title>get_weat</title>\n<ellipse fill=\"lightgreen\" stroke=\"black\" cx=\"229.5\" cy=\"-16.26\" rx=\"50.41\" ry=\"16.03\"/>\n<text text-anchor=\"middle\" x=\"229.5\" y=\"-12.56\" font-family=\"Arial\" font-size=\"14.00\">get_weat</text>\n</g>\n<!-- Weather Assistant&#45;&gt;get_weat -->\n<g id=\"edge3\" class=\"edge\">\n<title>Weather Assistant&#45;&gt;get_weat</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" stroke-dasharray=\"1,5\" d=\"M223.01,-68.36C222.62,-59.94 222.67,-50.77 223.16,-42.55\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"226.66,-42.71 224.06,-32.44 219.69,-42.09 226.66,-42.71\"/>\n</g>\n<!-- get_weat&#45;&gt;Weather Assistant -->\n<g id=\"edge4\" class=\"edge\">\n<title>get_weat&#45;&gt;Weather Assistant</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" stroke-dasharray=\"1,5\" d=\"M234.94,-32.44C235.86,-39.82 236.29,-49.05 236.25,-58.18\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"232.74,-58.27 235.99,-68.36 239.74,-58.45 232.74,-58.27\"/>\n</g>\n<!-- Panaversity Assistant &#45;&gt;__end__ -->\n<g id=\"edge7\" class=\"edge\">\n<title>Panaversity Assistant &#45;&gt;__end__</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M81.25,-68.36C83.26,-59.97 85.45,-50.84 87.42,-42.64\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"90.83,-43.43 89.75,-32.89 84.02,-41.8 90.83,-43.43\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7ef67c250e90>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLASS 13"
      ],
      "metadata": {
        "id": "hKkP93_hH-vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent , AsyncOpenAI , OpenAIChatCompletionsModel , set_default_openai_client, set_tracing_disabled, Runner\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "external_client= AsyncOpenAI (\n",
        "    api_key = userdata.get('GEMINI_API_KEY'),\n",
        "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "set_default_openai_client(external_client)\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    openai_client = external_client,\n",
        "    model=\"gemini-2.0-flash\")\n",
        "\n",
        "\n",
        "\n",
        "agnet = Agent(\n",
        "    name = 'General assistant',\n",
        "    instructions = 'You are a helpful assistant',\n",
        "    model = model\n",
        ")\n",
        "\n",
        "result = Runner.run_sync(agnet,\"hi how are you \")\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc776g2IIA8j",
        "outputId": "8a33123c-c001-4a3c-d84a-865db7289158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi there! As a large language model, I don't experience feelings in the same way humans do. But I'm functioning optimally and ready to assist you with any questions or tasks you have. How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agents.extensions.visualization import draw_graph\n",
        "draw_graph(agnet)"
      ],
      "metadata": {
        "id": "6KeCgUZT4POe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "7889a1f6-ecc6-47d4-d45e-972e798bbf3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"130pt\" height=\"203pt\"\n viewBox=\"0.00 0.00 130.00 203.05\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 199.05)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-199.05 126,-199.05 126,4 -4,4\"/>\n<!-- __start__ -->\n<g id=\"node1\" class=\"node\">\n<title>__start__</title>\n<ellipse fill=\"lightblue\" stroke=\"black\" cx=\"61\" cy=\"-178.79\" rx=\"51.74\" ry=\"16.03\"/>\n<text text-anchor=\"middle\" x=\"61\" y=\"-175.09\" font-family=\"Arial\" font-size=\"14.00\">__start__</text>\n</g>\n<!-- General assistant -->\n<g id=\"node3\" class=\"node\">\n<title>General assistant</title>\n<polygon fill=\"lightyellow\" stroke=\"black\" points=\"122,-126.53 0,-126.53 0,-68.53 122,-68.53 122,-126.53\"/>\n<text text-anchor=\"middle\" x=\"61\" y=\"-93.83\" font-family=\"Arial\" font-size=\"14.00\">General assistant</text>\n</g>\n<!-- __start__&#45;&gt;General assistant -->\n<g id=\"edge1\" class=\"edge\">\n<title>__start__&#45;&gt;General assistant</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M61,-162.51C61,-155.12 61,-145.89 61,-136.76\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"64.5,-136.6 61,-126.6 57.5,-136.6 64.5,-136.6\"/>\n</g>\n<!-- __end__ -->\n<g id=\"node2\" class=\"node\">\n<title>__end__</title>\n<ellipse fill=\"lightblue\" stroke=\"black\" cx=\"61\" cy=\"-16.26\" rx=\"48.58\" ry=\"16.03\"/>\n<text text-anchor=\"middle\" x=\"61\" y=\"-12.56\" font-family=\"Arial\" font-size=\"14.00\">__end__</text>\n</g>\n<!-- General assistant&#45;&gt;__end__ -->\n<g id=\"edge2\" class=\"edge\">\n<title>General assistant&#45;&gt;__end__</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M61,-68.36C61,-60.07 61,-51.04 61,-42.92\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"64.5,-42.89 61,-32.89 57.5,-42.89 64.5,-42.89\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7ef6a68514d0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "open AI uses responses API at their endpoint as default. we are converting it into ChatCompletions . we are setting configurations as global configuration so we dont need to set it up in every agent ."
      ],
      "metadata": {
        "id": "GjqDWonBZU5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import name\n",
        "from agents import Agent , AsyncOpenAI , OpenAIChatCompletionsModel , set_default_openai_client, set_tracing_disabled, Runner, set_default_openai_api\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "set_default_openai_api(\"chat_completions\")\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key = userdata.get('GEMINI_API_KEY'),\n",
        "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "set_default_openai_client(external_client)\n",
        "\n",
        "agent = Agent (name = \"Assistant \" , instructions = \"You are a helpful assistant\" , model = \"gemini-2.0-flash\")\n",
        "result = Runner.run_sync(agent, \"can u tell me the weather of karachi\")\n",
        "print(result.final_output)\n",
        "\n"
      ],
      "metadata": {
        "id": "ijV1849ArIfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d4205c-d116-46c1-da8e-f96ce5ad0bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay! To give you the most accurate weather forecast for Karachi, I need to know the specific date and time you're interested in. However, I can give you a general overview of the typical weather in Karachi and a quick look at today's forecast.\n",
            "\n",
            "**Typical Weather in Karachi:**\n",
            "\n",
            "*   Karachi generally has a hot and humid climate.\n",
            "*   **Summer (March to May):** Hot and humid, with temperatures often exceeding 35°C (95°F).\n",
            "*   **Monsoon (June to September):** High humidity and rainfall. Can experience heavy downpours and occasional flooding.\n",
            "*   **Winter (November to February):** Pleasant and dry, with temperatures ranging from 20°C to 28°C (68°F to 82°F) during the day and cooler nights.\n",
            "*   **October:** Considered a transition month with moderate temperatures.\n",
            "\n",
            "**For a more precise forecast, I recommend checking a reliable weather website or app. Here are a few popular options:**\n",
            "\n",
            "*   **Google Weather:** Just search \"weather in Karachi\" on Google.\n",
            "*   **AccuWeather:** [https://www.accuweather.com/](https://www.accuweather.com/)\n",
            "*   **The Weather Channel:** [https://weather.com/](https://weather.com/)\n",
            "\n",
            "When you check, look for:\n",
            "\n",
            "*   **Temperature:** Current temperature and the high/low for the day.\n",
            "*   **Conditions:** Sunny, cloudy, rainy, etc.\n",
            "*   **Humidity:** How humid it feels.\n",
            "*   **Wind:** Wind speed and direction.\n",
            "*   **Chance of rain:** Percentage chance of precipitation.\n",
            "\n",
            "I hope this helps! Let me know if you have any other questions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class 15\n"
      ],
      "metadata": {
        "id": "A6YkFu0N4IJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from agents import Agent, Runner, RunContextWrapper , OpenAIChatCompletionsModel, set_default_openai_client , function_tool ,AsyncOpenAI, set_tracing_disabled\n",
        "from google.colab import userdata\n",
        "import asyncio\n",
        "\n",
        "\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "external_client= AsyncOpenAI (\n",
        "    api_key = userdata.get('GEMINI_API_KEY'),\n",
        "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "set_default_openai_client(external_client)\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    openai_client = external_client,\n",
        "    model=\"gemini-2.0-flash\")\n",
        "\n",
        "\n",
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from agents import Agent, RunContextWrapper, Runner, function_tool\n",
        "\n",
        "@dataclass\n",
        "class UserInfo:\n",
        "    name: str\n",
        "    uid: int\n",
        "    age:int\n",
        "    location: str\n",
        "\n",
        "@function_tool\n",
        "async def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -> str:\n",
        "    \"\"\"Fetch the age of the user. Call this function to get user's age information.\"\"\"\n",
        "    return f\"The user {wrapper.context.name} is {wrapper.context.age}  years old, and he lives in {wrapper.context.location}\"\n",
        "\n",
        "\n",
        "@function_tool\n",
        "async def greet_user(wrapper: RunContextWrapper[UserInfo])->str :\n",
        "  return f\"Hello , {wrapper.context.name}\"\n",
        "\n",
        "\n",
        "\n",
        "@function_tool\n",
        "async def fetch_user_location(wrapper:RunContextWrapper[UserInfo]) -> str :\n",
        "  \"\"\"Fetch the location of the user. call the function to get all the information about the user\"\"\"\n",
        "  return f\"The user {wrapper.context.name} is from {wrapper.context.location}\"\n",
        "\n",
        "async def main():\n",
        "    user_info = UserInfo(name=\"John\", uid=123 ,  age = 47 , location=\"Pakistan\")\n",
        "\n",
        "    agent = Agent[UserInfo](\n",
        "        name=\"Assistant\",\n",
        "        instructions = \"first you have to greet the user by using <function_call>greet_user<function_call> and welcomr tehm to the panaversity !you have to tell about the user all of his information\" ,\n",
        "        tools=[fetch_user_age, fetch_user_location , greet_user],\n",
        "        model=model,\n",
        "    )\n",
        "\n",
        "    result = await Runner.run(\n",
        "        starting_agent=agent,\n",
        "        input=\"What is the age of the user? and his current location\",\n",
        "        context=user_info,\n",
        "    )\n",
        "\n",
        "    print(result.final_output)\n",
        "    # The user John is 47 years old.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "\n",
        "\n",
        "\n",
        "# @dataclass\n",
        "# class UserInfo:\n",
        "#   name:str\n",
        "#   uid:str\n",
        "#   age: int\n",
        "#   location:str = \"Pakistan\"\n",
        "\n",
        "\n",
        "# @function_tool\n",
        "# async def fetch_user_age(wrapper:RunContextWrapper[UserInfo]) -> str :\n",
        "#   \"\"\" Returns the age of the user \"\"\"\n",
        "#   print (\"[-> Tool]\" , wrapper)\n",
        "#   return f\"User {wrapper.context.name} is {wrapper.context.age} years old\"\n",
        "\n",
        "\n",
        "# async def main ():\n",
        "#   userinfo = UserInfo(name=\"John Doe\", uid=\"123\"  , age = 20)\n",
        "\n",
        "#   agent = Agent[UserInfo](\n",
        "#       name = \"Assistant\",\n",
        "#       tools =[fetch_user_age],\n",
        "#       model = model\n",
        "#   )\n",
        "\n",
        "#   result = await Runner.run(\n",
        "#     starting_agent = agent,\n",
        "#     input = \"what is the age of the user ?\" ,\n",
        "#     context=UserInfo\n",
        "# )\n",
        "\n",
        "\n",
        "#   print(result.final_output)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#   asyncio.run(main())\n"
      ],
      "metadata": {
        "id": "enKPqtZ3aMbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74fc67b5-6be5-4910-c816-80b53563f821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have information about you. You are 47 years old and you are from Pakistan. Welcome to Panaversity!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WHuGuZFObbXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STREAMING"
      ],
      "metadata": {
        "id": "q9XdO4Lqbenq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple streaming(run_stream) and its events(printing all its events what stream does)"
      ],
      "metadata": {
        "id": "xZ-MZIh8v8zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "async def main():\n",
        "  agent = Agent(\n",
        "      name=\"Joker\",\n",
        "      instructions = \"You are a helpful assistant \",\n",
        "      model = model\n",
        "  )\n",
        "\n",
        "\n",
        "  result = Runner.run_streamed(agent,  input= \"please tell me 5 jokes\")\n",
        "  async for event in result.stream_events():\n",
        "    if event.type == \"raw_response_event\" and isinstance(event.data , ResponseTextDeltaEvent):\n",
        "      print (event.data.delta , end = \"\" , flush=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "diHpqcqrTDGa",
        "outputId": "681486fd-3c90-4235-913f-44e881c51e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'RunItemStreamEvent' object has no attribute 'data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-39031256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-39031256.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_streamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"please tell me 5 jokes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"run_item_stream_event\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mResponseTextDeltaEvent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RunItemStreamEvent' object has no attribute 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from agents import Agent, ItemHelpers, Runner, function_tool\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def how_many_jokes():\n",
        "  return random.randint(1,10)\n",
        "\n",
        "\n",
        "async def main():\n",
        "  agent = Agent(\n",
        "      name= \"Joker\",\n",
        "      instructions = \"First call the tool 'how_many_jokes' then the tell the  jokes to the user \",\n",
        "      tools = [how_many_jokes],\n",
        "      model = model\n",
        "  )\n",
        "\n",
        "  result=Runner.run_streamed(agent , input = \"hello, tell me some jokes \")\n",
        "\n",
        "\n",
        "\n",
        "  print(\"===============Run starting ================\")\n",
        "  async for event in result.stream_events():\n",
        "    print (event)\n",
        "\n",
        "\n",
        "if __name__== '__main__':\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN-3tKD59UOZ",
        "outputId": "46c37d07-49b2-409a-f5ad-34356e5c1ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============Run starting ================\n",
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7aff2ab9d8a0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions=\"First call the tool 'how_many_jokes' then the tell the  jokes to the user \", prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7aff546dd5d0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1754543852.8048723, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=None, safety_identifier=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{}', item_id='__fake_id__', output_index=0, sequence_number=2, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), output_index=0, sequence_number=3, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1754543852.8048723, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None)], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=None, safety_identifier=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=4, type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7aff2ab9d8a0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions=\"First call the tool 'how_many_jokes' then the tell the  jokes to the user \", prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7aff546dd5d0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), type='tool_call_item'), type='run_item_stream_event')\n",
            "RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7aff2ab9d8a0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions=\"First call the tool 'how_many_jokes' then the tell the  jokes to the user \", prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7aff546dd5d0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item={'call_id': '', 'output': '9', 'type': 'function_call_output'}, output=9, type='tool_call_output_item'), type='run_item_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1754543853.1989253, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=None, safety_identifier=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=None), sequence_number=2, type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='I am', item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=3, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\" programmed to tell jokes, but I don't have any specific jokes to tell at\", item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' this time. I can tell you that I have access to 9 jokes. Please', item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ask me again later.\\n', item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=6, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text=\"I am programmed to tell jokes, but I don't have any specific jokes to tell at this time. I can tell you that I have access to 9 jokes. Please ask me again later.\\n\", type='output_text', logprobs=None), sequence_number=7, type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"I am programmed to tell jokes, but I don't have any specific jokes to tell at this time. I can tell you that I have access to 9 jokes. Please ask me again later.\\n\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), output_index=0, sequence_number=8, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1754543853.1989253, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"I am programmed to tell jokes, but I don't have any specific jokes to tell at this time. I can tell you that I have access to 9 jokes. Please ask me again later.\\n\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=None, safety_identifier=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=9, type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7aff2ab9d8a0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions=\"First call the tool 'how_many_jokes' then the tell the  jokes to the user \", prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7aff546dd5d0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"I am programmed to tell jokes, but I don't have any specific jokes to tell at this time. I can tell you that I have access to 9 jokes. Please ask me again later.\\n\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we are attaching a tool to see which events occur\n",
        "import random\n",
        "from agents import Agent, ItemHelpers, Runner, function_tool\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def how_many_jokes():\n",
        "  return random.randint(1,10)\n",
        "\n",
        "\n",
        "async def main():\n",
        "  agent = Agent(\n",
        "      name= \"Joker\",\n",
        "      instructions = \"First call the tool 'how_many_jokes' then the tell the  jokes to the user \",\n",
        "      tools = [how_many_jokes],\n",
        "      model = model\n",
        "  )\n",
        "\n",
        "  result=Runner.run_streamed(agent , input = \"hello, tell me some jokes \")\n",
        "\n",
        "\n",
        "\n",
        "  print(\"===============Run starting ================\")\n",
        "  async for event in result.stream_events():\n",
        "        # We'll ignore the raw responses event deltas\n",
        "        if event.type == \"raw_response_event\":\n",
        "            continue\n",
        "        elif event.type == \"agent_updated_stream_event\":\n",
        "            print(f\"Agent updated: {event.new_agent.name}\")\n",
        "            continue\n",
        "        elif event.type == \"run_item_stream_event\":\n",
        "            if event.item.type == \"tool_call_item\":\n",
        "                print(\"-- Tool was called\")\n",
        "            elif event.item.type == \"tool_call_output_item\":\n",
        "                print(f\"-- Tool output: {event.item.output}\")\n",
        "            elif event.item.type == \"message_output_item\":\n",
        "                print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n",
        "            else:\n",
        "                pass  # Ignore other event types\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYZ3mMqK2sXr",
        "outputId": "6ed7a22f-6408-42f3-9776-635a87a12080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============Run starting ================\n",
            "Agent updated: Joker\n",
            "-- Tool was called\n",
            "-- Tool output: 6\n",
            "-- Message output:\n",
            " I am programmed to tell 6 jokes. Here they are:\n",
            "\n",
            "1. Why don't scientists trust atoms? Because they make up everything!\n",
            "2. Parallel lines have so much in common. It’s a shame they’ll never meet.\n",
            "3. Why did the scarecrow win an award? Because he was outstanding in his field!\n",
            "4. I told my wife she was drawing her eyebrows too high. She seemed surprised.\n",
            "5. What do you call a fish with no eyes? Fsh!\n",
            "6. Why don't eggs tell jokes? They'd crack each other up!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "CLASS 16\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "H0F1ltmNmLO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "from agents import Agent, Runner\n",
        "\n",
        "class MathHomeworkOutput(BaseModel):\n",
        "   is_math_Homework :bool\n",
        "   reasoning :str\n",
        "   answer :str\n",
        "\n",
        "\n",
        "guardrail_agent = Agent(\n",
        "    name = \"Guardrail check\",\n",
        "    instructions = \"Check if the user is asking you abot their math homework\",\n",
        "    output_type = MathHomeworkOutput,\n",
        "    model = model\n",
        ")\n"
      ],
      "metadata": {
        "id": "aIcjcyFsmOXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = Runner.run_sync(guardrail_agent,\"What is 2+2?\")\n",
        "print(output.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Givaf8atyVU",
        "outputId": "b65541ea-5756-4f25-9d63-fe77f4497bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is_math_Homework=True reasoning='This is a basic arithmetic question that is often given as a simple math problem in early education. Therefore, it can be categorized as math homework.' answer='4'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xsfqmn7y2jcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import input_guardrail, TResponseInputItem, GuardrailFunctionOutput\n",
        "from agents import Agent, Runner, RunContextWrapper, InputGuardrailTripwireTriggered\n",
        "\n",
        "@input_guardrail\n",
        "async def math_guardrail(\n",
        "    ctx:RunContextWrapper[None] , agent:Agent , input:str|list[TResponseInputItem])->GuardrailFunctionOutput:\n",
        "    result = await Runner.run(guardrail_agent , input , context = ctx.context )\n",
        "\n",
        "    return GuardrailFunctionOutput(\n",
        "        output_info = result.final_output,\n",
        "        tripwire_triggered = result.final_output.is_math_Homework\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "WPgnGMGyuUji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent (\n",
        "    name = \"Math Home work Assistant\",\n",
        "    instructions= \"You are a math support agent . You help students with their questions\",\n",
        "    input_guardrails=[math_guardrail],\n",
        "    model=model\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "k3v5FyNn1--E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  result  = await Runner.run(agent , \"Hello can u solve for me x:2x+3=11\")\n",
        "  print (\"Gusrdrail didn't trip this is unexpected\")\n",
        "  print (result.final_output)\n",
        "\n",
        "except InputGuardrailTripwireTriggered:\n",
        "  print (\"Math homework guardrail tripped\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6FoK5L23Oyu",
        "outputId": "17ef31c1-8d8a-4954-8883-f34f6a3afb4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Math homework guardrail tripped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  result  = await Runner.run(agent , \"Hello\")\n",
        "  print (\"Gusrdrail didn't trip this is unexpected\")\n",
        "  print (result.final_output)\n",
        "\n",
        "except InputGuardrailTripwireTriggered:\n",
        "  print (\"Math homework guardrail tripped\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Fn-Ns6o4ahu",
        "outputId": "56c2d6ee-8c32-4857-ea2a-9381026836bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gusrdrail didn't trip this is unexpected\n",
            "Hi there! How can I help you with math today? Do you have a specific question or topic you're working on?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT GUARDRAIL"
      ],
      "metadata": {
        "id": "hkjdKa5zIRLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CountryOutput(BaseModel):\n",
        "  is_country_allowed:bool\n",
        "  reason:str\n",
        "  country:str\n",
        "  answer:str\n",
        "\n",
        "\n",
        "country_guardrail_agent = Agent (\n",
        "    name = \"COuntry Guradrail Check\",\n",
        "    instructions = \"We only allow to talk about Pakistan\",\n",
        "    output_type = CountryOutput,\n",
        "    model = model\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "q1BozxzM4wbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = Runner.run_sync(country_guardrail_agent,\"who is the founder of Pakistan\")\n",
        "print(response.final_output.model_dump())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrP0Rw5rIQvB",
        "outputId": "0343155a-9c03-4144-d92d-7755a7dd1327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'is_country_allowed': True, 'reason': 'The question is about the founder of Pakistan, which is within the allowed topic.', 'country': 'Pakistan', 'answer': 'Muhammad Ali Jinnah'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import output_guardrail , OutputGuardrailTripwireTriggered\n",
        "\n",
        "@output_guardrail\n",
        "async def guardrail_output(\n",
        "    ctx:RunContextWrapper , agent :Agent ,output :CountryOutput\n",
        ")->GuardrailFunctionOutput:\n",
        "\n",
        "  result= await Runner.run (country_guardrail_agent , output ,context = ctx.context)\n",
        "  print (\"\\n\\n[GUARDRAIL_Response] \" , result.final_output, \"\\n\\n\")\n",
        "\n",
        "  return GuardrailFunctionOutput(\n",
        "      output_info = result.final_output,\n",
        "      tripwire_triggered = result.final_output.is_country_allowed is False\n",
        "  )"
      ],
      "metadata": {
        "id": "lCxZtt2EI2vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\n",
        "    name = \"English Homework Assistant\",\n",
        "    instructions = \"You are a english support agent.  yOu help student with their questions \",\n",
        "    # input_guardrails=[math_guardrail],\n",
        "    output_guardrails=[guardrail_output],\n",
        "    model=model\n",
        ")"
      ],
      "metadata": {
        "id": "RmWbQTDBNvKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  output_guardrail_result = await Runner.run(agent , \"how many provinces in Pakistan \")\n",
        "  print(output_guardrail_result.final_output)\n",
        "\n",
        "\n",
        "except OutputGuardrailTripwireTriggered as e :\n",
        "  print(\"Math homework guardrail tripped\",e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T36EGJjAONrf",
        "outputId": "19897cc4-54c2-46a3-c0e1-56ba0e5b301f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[GUARDRAIL_Response]  is_country_allowed=True reason='The information provided is about the provinces and territories within Pakistan, which aligns with the allowed topic.' country='Pakistan' answer='Pakistan has four provinces: Punjab, Sindh, Khyber Pakhtunkhwa (KPK), and Balochistan. It also includes Islamabad Capital Territory, Gilgit-Baltistan, and Azad Jammu and Kashmir.' \n",
            "\n",
            "\n",
            "There are **four** provinces in Pakistan:\n",
            "\n",
            "*   **Punjab**\n",
            "*   **Sindh**\n",
            "*   **Khyber Pakhtunkhwa (KPK)**\n",
            "*   **Balochistan**\n",
            "\n",
            "It's also worth noting that Pakistan has other territories, including:\n",
            "\n",
            "*   **Islamabad Capital Territory:** This is where the capital city, Islamabad, is located.\n",
            "*   **Gilgit-Baltistan:** An administrative territory in the northern part of the country.\n",
            "*   **Azad Jammu and Kashmir:** A self-governing region that Pakistan administers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASS 17\n"
      ],
      "metadata": {
        "id": "QbFGMcSrHKYA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b3UoBHFTOfgT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}